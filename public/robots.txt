# Robots.txt for pdfo.dev - Online PDF Tools
# Generated for optimal SEO and search engine indexing

# Default crawling rules for all robots
User-agent: *
Allow: /

# Block sensitive backend paths
Disallow: /api/
Disallow: /uploads/
Disallow: /admin/
Disallow: /tmp/

# Block temporary and cache files
Disallow: /*.tmp$
Disallow: /*.cache$
Disallow: /*.log$

# Block development and testing paths
Disallow: /dev/
Disallow: /test/
Disallow: /.git/
Disallow: /node_modules/

# Set crawl delay (1 second) for respectful crawling
Crawl-delay: 1

# Allow specific search engine bots with optimized settings
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 1

# Allow social media crawlers for better sharing
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

User-agent: WhatsApp
Allow: /

User-agent: Telegram
Allow: /

# Allow Google AdSense crawlers
User-agent: Mediapartners-Google
Allow: /

User-agent: AdsBot-Google
Allow: /

User-agent: Googlebot-Image
Allow: /

User-agent: Googlebot-News
Allow: /

User-agent: Googlebot-Video
Allow: /

# Block problematic or aggressive crawlers
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

# Sitemap location
Sitemap: https://pdfo.dev/sitemap.xml